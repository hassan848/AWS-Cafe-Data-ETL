# AWS/Cafe-Data-ETL/Project Background
An ETL Pipeline using AWS to process data from multiple branches of a café business and generating meaningful business reports/visualisations.

In this project, I was required to make an upgrade to an existing data processing workflow for a cafe business. The previous client application simply consisted of a CSV file of transactional data for a single given branch to be generated for that day. the data would then be stored to a local computer where some sale figures could be created.

# The Problem 
Due to recent success and the opening of multiple other branches, the client needed a better way to bring together data collated from all their stores, analyse the data, identify trends and produce insights in a timely manner; which will all help them to decide how to target new and returning customers.

# The Solution
Building a fully scalable ETL (Extract, Transform, Load) pipeline to handle large volumes of transaction data for the business. This pipeline will collect all the transaction data generated by each individual café and place it into a Redshift database. 

This pipeline will collect all the transaction data generated by each individual café, transforn the data while removing sesitive information, and then load it into the database. By being able to easily query the company's data as a whole, the client will drastically increase their ability to identify company-wide trends and insights.

I first created the pipeline locally on my local PC in python to extract and transform the data being loaded into a local Postgres database. the databse schema was normalised to 4nf (see schema in repo) so the data had to be also normalised in the transformation page. The postgres db was hosted on Docker.

I then moved this ETL pipeline to the cloud - AWS (see flowchart diagram in repo) where the initial dirty data was being sent to an S3 bucket (simple storage service), the Python code responsible for Extraction, Transformation and Loading is now running on the cloud, split between two lambda functions (serverless execution, event driven) and loaded to a Redshift database. 

I then created the business visualisations of all the collated data from all branches using Grafana which was hosted on a virtual server on the cloud (AWS EC2) - these visualisations of the data were then presented to the client. Application monitoring software is also used to produce operational metrics, such as system errors, up-time and more - I also created visualisations for these metrics on Grafana.

This ETL pipeline is automated so as long new dirty data from the cafe branches are uploaded to the S3 bucket which sets of a trigger to invoke the lambda functions which then loads the clean, normalised data to the database.

I also created the IaC (Infrastructure as Code) to automate the infrastructure build of the cloud services e.g. Lambda functions that has an S3 trigger. I did this through a combination of Yaml files and a Bash script. 

# Data Engineering Skills used:
* Data Warehousing
* ETL Pipeline building
* Data Analytics / Visualisations / Business Intelligence
* Monitoring
* Development Operations (DevOps)
